{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WQD7005 - Data Mining\n",
    "## Lab Exersice 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Number : 17043640\n",
    "\n",
    "#### Name                 : Gunasegarran Magadevan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Prequisite\n",
    "Perform the following steps before trying the lab test 2:\n",
    "\n",
    "Import pandas as \"pd\" and load the house price dataset into \"df\".\n",
    "Print dataset information to refresh your memory.\n",
    "Run preprocess_data function on the dataframe to perform preprocessing steps \n",
    "discussed last week (lab 1 dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TargetB</th>\n",
       "      <th>ID</th>\n",
       "      <th>TargetD</th>\n",
       "      <th>GiftCnt36</th>\n",
       "      <th>GiftCntAll</th>\n",
       "      <th>GiftCntCard36</th>\n",
       "      <th>GiftCntCardAll</th>\n",
       "      <th>GiftAvgLast</th>\n",
       "      <th>GiftAvg36</th>\n",
       "      <th>GiftAvgAll</th>\n",
       "      <th>...</th>\n",
       "      <th>PromCntCardAll</th>\n",
       "      <th>StatusCat96NK</th>\n",
       "      <th>StatusCatStarAll</th>\n",
       "      <th>DemCluster</th>\n",
       "      <th>DemAge</th>\n",
       "      <th>DemGender</th>\n",
       "      <th>DemHomeOwner</th>\n",
       "      <th>DemMedHomeValue</th>\n",
       "      <th>DemPctVeterans</th>\n",
       "      <th>DemMedIncome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.50</td>\n",
       "      <td>9.25</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>15.88</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>67.0</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>186800</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46110</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.17</td>\n",
       "      <td>3.73</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>87600</td>\n",
       "      <td>36</td>\n",
       "      <td>38750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>185937</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>139200</td>\n",
       "      <td>27</td>\n",
       "      <td>38942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>29637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>53.0</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>168100</td>\n",
       "      <td>37</td>\n",
       "      <td>71509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TargetB      ID  TargetD  GiftCnt36  GiftCntAll  GiftCntCard36  \\\n",
       "0        0   14974      NaN          2           4              1   \n",
       "1        0    6294      NaN          1           8              0   \n",
       "2        1   46110      4.0          6          41              3   \n",
       "3        1  185937     10.0          3          12              3   \n",
       "4        0   29637      NaN          1           1              1   \n",
       "\n",
       "   GiftCntCardAll  GiftAvgLast  GiftAvg36  GiftAvgAll  ...  PromCntCardAll  \\\n",
       "0               3         17.0      13.50        9.25  ...              13   \n",
       "1               3         20.0      20.00       15.88  ...              24   \n",
       "2              20          6.0       5.17        3.73  ...              22   \n",
       "3               8         10.0       8.67        8.50  ...              16   \n",
       "4               1         20.0      20.00       20.00  ...               6   \n",
       "\n",
       "   StatusCat96NK  StatusCatStarAll  DemCluster  DemAge  DemGender  \\\n",
       "0              A                 0           0     NaN          F   \n",
       "1              A                 0          23    67.0          F   \n",
       "2              S                 1           0     NaN          M   \n",
       "3              E                 1           0     NaN          M   \n",
       "4              F                 0          35    53.0          M   \n",
       "\n",
       "   DemHomeOwner  DemMedHomeValue  DemPctVeterans DemMedIncome  \n",
       "0             U                0               0            0  \n",
       "1             U           186800              85            0  \n",
       "2             U            87600              36        38750  \n",
       "3             U           139200              27        38942  \n",
       "4             U           168100              37        71509  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    " # read the dataset\n",
    "df = pd.read_csv('lab1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def data_prep():\n",
    "    \n",
    "    # change DemCluster from interval/integer to nominal/str\n",
    "    df['DemCluster'] = df['DemCluster'].astype(str)\n",
    "    \n",
    "    # change DemHomeOwner into binary 0/1 variable\n",
    "    dem_home_owner_map = {'U':0, 'H': 1}\n",
    "    df['DemHomeOwner'] = df['DemHomeOwner'].map(dem_home_owner_map)\n",
    "    \n",
    "    # denote errorneous values in DemMidIncome\n",
    "    mask = df['DemMedIncome'] < 1\n",
    "    df.loc[mask, 'DemMedIncome'] = np.nan\n",
    "    \n",
    "    # impute missing values in DemAge with its mean\n",
    "    df['DemAge'].fillna(df['DemAge'].mean(), inplace=True)\n",
    "\n",
    "    # impute med income using mean\n",
    "    df['DemMedIncome'].fillna(df['DemMedIncome'].mean(), inplace=True)\n",
    "\n",
    "    # impute gift avg card 36 using mean\n",
    "    df['GiftAvgCard36'].fillna(df['GiftAvgCard36'].mean(), inplace=True)\n",
    "    \n",
    "    # drop ID and the unused target variable\n",
    "    df.drop(['ID', 'TargetD'], axis=1, inplace=True)\n",
    "    \n",
    "    # one-hot encoding\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto import the python function from Lab_Exercise_2 path\n",
    "from dm_tools import data_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Partitioning\n",
    "Perform following operations and answer the following questions:\n",
    "\n",
    "##### a) Describe training, validation and test dataset.\n",
    "##### b) What is the purpose for each of these split?\n",
    "- Training dataset: is a set of examples that is used to build the model. Once the models are built, we need to know how good they are.\n",
    "- Validation dataset: is a set of examples that is used to evaluate the performance of a model. Validation dataset is unseen (not used in training/fitting process) and typically has similar distribution with training dataset. Validation dataset is commonly used to estimate performance and choose one of a number of different models. The combined performance on training and validation sets also reveal if the learned model is overfit to the training dataset. In other words, it reveals if the model has learnt relationships specific to the provided data which might not be true in general. Overfitting model tends to perform horribly outside of the training set. We will see the example of overfitting in this practical note.\n",
    "- Test dataset: is a set of examples used to estimate the performance of a model in practice. Similar to validation, test dataset is unseen. However, it is not used in model selection process.\n",
    "    \n",
    "#### c) What is k-fold cross validation? \n",
    "- A common problem with using validation dataset is we drastically reduce the number of samples for training the model. In addition, random split used for train/validation can impact the model selected during training process. To solve this problem, k-fold cross-validation (k-fold CV) is commonly used.\n",
    " - In k-fold cross-validation, an exclusive validation dataset is no longer required. Instead, the training dataset is randomly partitioned into $k$ equal-sized partitions. Of the $k$ subsamples, a single subsample is retained as the validation data for testing the model, and the remaining $k-1$ subsamples are used as training data. The cross-validation process is then repeated $k$ times (the folds), with each of the $k$ subsamples used exactly once as the validation data. The $k$ results from the folds can then be averaged (or otherwise combined) to produce a single estimation of model performance.\n",
    "    \n",
    "#### d) What is the advantage and disadvantage of k-fold CV compared to normal training/test/validation method?\n",
    "- Cross-validation allows the validation process to generalise better (does not depend on randomness of the train/validation split) and reduces data waste (very beneficial for limited size datasets). The drawback of CV is the computation can be expensive and slow as it multiplies the model training time by $k$ times. We will learn more about CV later in this tutorial through the usage of GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing step\n",
    "df = data_prep()\n",
    "\n",
    "# target/input split\n",
    "y = df['TargetB']\n",
    "X = df.drop(['TargetB'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'as_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-612d68420ec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'as_matrix'"
     ]
    }
   ],
   "source": [
    "# setting random state\n",
    "rs = 10\n",
    "\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Decision Tree\n",
    "Perform the following operations and answer the question.\n",
    "\n",
    "##### a) Import and build a decision tree classifier. \n",
    "##### b) Set the random state to 0 to ensure your result is similar with the answers. \n",
    "##### c) Fit it against the training data.\n",
    "##### d) What is the performance of the model against training data? \n",
    "##### e) How about against the test data? Do you see any indication of overfitting here?\n",
    "##### f) What are the top 5 most important features in this model?\n",
    "##### g) Visualise the structure of your decision tree. Can you identify characteristics of important features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# simple decision tree training\n",
    "model = DecisionTreeClassifier(random_state=rs)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy:\", model.score(X_train, y_train), \", Test accuracy:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# grab feature importances from the model and feature name from the original X\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(importances)\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', importances[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# visualize\n",
    "dotfile = StringIO()\n",
    "export_graphviz(model, out_file=dotfile, feature_names=X.columns)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph[0].write_png(\"week3_dt_viz.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain with a small max_depth limit\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=3, random_state=rs)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(importances)\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', importances[i])\n",
    "\n",
    "# visualize\n",
    "dotfile = StringIO()\n",
    "export_graphviz(model, out_file=dotfile, feature_names=X.columns)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph[0].write_png(\"week3_dt_viz.png\") # saved in the following file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = []\n",
    "train_score = []\n",
    "\n",
    "# check the model performance for max depth from 2-20\n",
    "for max_depth in range(2, 21):\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth, random_state=rs)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_score.append(model.score(X_test, y_test))\n",
    "    train_score.append(model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot max depth hyperparameter values vs training and test accuracy score\n",
    "plt.plot(range(2, 21), train_score, 'b', range(2,21), test_score, 'r')\n",
    "plt.xlabel('max_depth\\nBlue = training acc. Red = test acc.')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search CV\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(2, 7),\n",
    "          'min_samples_leaf': range(20, 60, 10)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search CV #2\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(2, 6),\n",
    "          'min_samples_leaf': range(45, 56)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydot\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "def analyse_feature_importance(dm_model, feature_names, n_to_display=20):\n",
    "    # grab feature importances from the model\n",
    "    importances = dm_model.feature_importances_\n",
    "    \n",
    "    # sort them out in descending order\n",
    "    indices = np.argsort(importances)\n",
    "    indices = np.flip(indices, axis=0)\n",
    "\n",
    "    # limit to 20 features, you can leave this out to print out everything\n",
    "    indices = indices[:n_to_display]\n",
    "\n",
    "    for i in indices:\n",
    "        print(feature_names[i], ':', importances[i])\n",
    "\n",
    "def visualize_decision_tree(dm_model, feature_names, save_name):\n",
    "    dotfile = StringIO()\n",
    "    export_graphviz(dm_model, out_file=dotfile, feature_names=feature_names)\n",
    "    graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "    graph[0].write_png(save_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the feature importance and visualization analysis on GridSearchCV's best model\n",
    "from dm_tools import analyse_feature_importance, visualize_decision_tree\n",
    "\n",
    "analyse_feature_importance(cv.best_estimator_, X.columns, 20)\n",
    "visualize_decision_tree(cv.best_estimator_, X.columns, \"optimal_tree.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
