{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 3: Decision Tree Mining\n",
    "\n",
    "### In this practical\n",
    "1. [Resuming from practical 2](#resume)\n",
    "2. [Building your first decision tree model](#build)\n",
    "3. [Understanding and visualising your decision tree](#viz)\n",
    "4. [Finding optimal hyperparameters with GridSearchCV](#gridsearch)\n",
    "\n",
    "---\n",
    "\n",
    "**Written by Hendi Lie (h2.lie@qut.edu.au) and Richi Nayak (r.nayak@qut.edu.au). All rights reserved.**\n",
    "\n",
    "This practical note introduces you to predictive modelling using Decision Tree in Python. A decision tree is relatively simple model, yet it can be powerful and accurate if built and utilised properly. The decision tree model will help to classify the lapsing donors based on their responses to greeting card mailing conducted by the national veteran organisation.\n",
    "\n",
    "Predictive modelling, including decision trees, start with a training data set. Observations in a training data set are known as *training cases/samples/rows/instances/records*. The variables are called *inputs* (or *variables/attributes/columns/features/explanatory variables or independent variables*) and *targets* (or *response/outcome/output/class/dependent variables*). For a given case, input variables are used to estimate target variable. As you learned in practical 2, measurement type/role for inputs and target can be varied as the underlying problem requirement.\n",
    "\n",
    "As discussed in the lecture, there are two types of predictive modelling, classification and regression. Classification modelling aims to predict class labels (categorical/binary target), such as eligibility of clients for a loan (YES/NO - binary) or topic of a news article (sports, politics, entertainment - multi-class categorical). Regression aims to predict numerical/continous variable, such as age (1-100) or house price (\\$\\$\\$). We will be building predictive models to predict TARGETB, which is a binary variable, thus this practical task will introduce you to classification task.\n",
    "\n",
    "## 1. Resuming from practical 2 <a name=\"resume\"></a>\n",
    "\n",
    "In the last practical, we learned how to perform data preparation on a dataset for conducting meaningful data analytics. We learned how to build plots of data distribution, dealing with noisy values, imputing missing values and selecting inputs for building models. After practical 2, your code should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inside dm_tools.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def data_prep():\n",
    "    # read the veteran dataset\n",
    "    df = pd.read_csv('lab1.csv')\n",
    "    \n",
    "    # change DemCluster from interval/integer to nominal/str\n",
    "    df['DemCluster'] = df['DemCluster'].astype(str)\n",
    "    \n",
    "    # change DemHomeOwner into binary 0/1 variable\n",
    "    dem_home_owner_map = {'U':0, 'H': 1}\n",
    "    df['DemHomeOwner'] = df['DemHomeOwner'].map(dem_home_owner_map)\n",
    "    \n",
    "    # denote errorneous values in DemMidIncome\n",
    "    mask = df['DemMedIncome'] < 1\n",
    "    df.loc[mask, 'DemMedIncome'] = np.nan\n",
    "    \n",
    "    # impute missing values in DemAge with its mean\n",
    "    df['DemAge'].fillna(df['DemAge'].mean(), inplace=True)\n",
    "\n",
    "    # impute med income using mean\n",
    "    df['DemMedIncome'].fillna(df['DemMedIncome'].mean(), inplace=True)\n",
    "\n",
    "    # impute gift avg card 36 using mean\n",
    "    df['GiftAvgCard36'].fillna(df['GiftAvgCard36'].mean(), inplace=True)\n",
    "    \n",
    "    # drop ID and the unused target variable\n",
    "    df.drop(['ID', 'TargetD'], axis=1, inplace=True)\n",
    "    \n",
    "    # one-hot encoding\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto import from Lab_Exercise_2 path\n",
    "from dm_tools import data_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building your first decision tree <a name=\"build\"></a>\n",
    "\n",
    "### 2.1. Data partitioning\n",
    "\n",
    "Recall back to the lecture. In predictive modelling, there are 3 common steps taken:\n",
    "1. Model construction/training/learning\n",
    "2. Model evaluation/testing\n",
    "3. Model usage.\n",
    "\n",
    "To construct and evaluate models, we require 3 data partitions: training, validation and test dataset.\n",
    "* **Training dataset** is a set of examples that is used to build the model. Once the models are built, we need to know how good they are.\n",
    "* **Validation dataset** is a set of examples that is used to evaluate the performance of a model. Validation dataset is unseen (not used in training/fitting process) and typically has similar distribution with training dataset. Validation dataset is commonly used to estimate performance and choose one of a number of different models. The combined performance on training and validation sets also reveal if the learned model is **overfit** to the training dataset. In other words, it reveals if the model has learnt relationships specific to the provided data which might not be true in general. Overfitting model tends to perform horribly outside of the training set. We will see the example of overfitting in this practical note.\n",
    "* **Test dataset** is a set of examples used to estimate the performance of a model in practice. Similar to validation, test dataset is unseen. However, it is not used in model selection process.\n",
    "\n",
    "A common problem with using validation dataset is we drastically reduce the number of samples for training the model. In addition, random split used for train/validation can impact the model selected during training process. To solve this problem, k-fold cross-validation (k-fold CV) is commonly used.\n",
    "\n",
    "In k-fold cross-validation, an exclusive validation dataset is no longer required. Instead, the training dataset is randomly partitioned into $k$ equal-sized partitions. Of the $k$ subsamples, a single subsample is retained as the validation data for testing the model, and the remaining $k-1$ subsamples are used as training data. The cross-validation process is then repeated $k$ times (the folds), with each of the $k$ subsamples used exactly once as the validation data. The $k$ results from the folds can then be averaged (or otherwise combined) to produce a single estimation of model performance.\n",
    "\n",
    "![Visualisation of $k$-fold cross validation](https://cdn-images-1.medium.com/max/1600/1*J2B_bcbd1-s1kpWOu_FZrg.png)\n",
    "*Visualisation of $k$-fold cross validation - credit to Adi Bronhstein*\n",
    "\n",
    "Cross-validation allows the validation process to generalise better (does not depend on randomness of the train/validation split) and reduces data waste (very beneficial for limited size datasets). The drawback of CV is the computation can be expensive and slow as it multiplies the model training time by $k$ times. We will learn more about CV later in this tutorial through the usage of GridSearchCV.\n",
    "\n",
    "In this practical, we will implement cross validation method. Firstly, we need to split the dataset into train and test set. In `sklearn`, this is commonly done using `train_test_split` function from `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the function is imported, we can start partitioning the dataset. The convention in Python is to assign input variables as `X` and target as `y`. In our case, `y` would be `TargetB` and `X` would be the rest of the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing step\n",
    "df = data_prep()\n",
    "\n",
    "# target/input split\n",
    "y = df['TargetB']\n",
    "X = df.drop(['TargetB'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, convert `X` (which is still a pandas `DataFrame` object) into a `numpy` matrix that can be consumed by `sklearn`. This step can be done using `.as_matrix()` function from pandas DataFrame.\n",
    "\n",
    "There are many configurations for dividing train and test data for CV. Some common configurations including 67/33 and 70/30.\n",
    "In this practical, we will split the data into 70% training dataset and 30% test data using the `.train_test_split()`.\n",
    "\n",
    "In performing the split, we also need to stratify based on `y`. Stratification ensures the same ratio of positive and negative targets in both train and test data set.\n",
    "\n",
    "As `train_test_split` shuffles the dataset before splitting it, it is important to set a consistent random state, which is the seed number used to generate the shuffle. We will use random seed of 10 in all practicals in this unit. In practice, you can use any integer number as long as it is consistent.\n",
    "\n",
    "> #### Seed and random state\n",
    "> In computer science, random seed (supplied to `random_state` hyperparameter in sklearn functions) is a number used to initialise pseudo-random number generator. Certain aspects of data mining/machine learning algorithms rely on randomness, such as shuffling dataset and initialising weights for gradient descent in logistic regression and decision tree. A consistent random seed ensures the results of running an algorithm consistent along multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'as_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-612d68420ec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'as_matrix'"
     ]
    }
   ],
   "source": [
    "# setting random state\n",
    "rs = 10\n",
    "\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Model construction and evaluation\n",
    "Once the data partitions are set up, we are ready to fit our first model to the training data. Let's start by importing `DecisionTreeClassifier`, initialise a model, and then training it using `.fit` function. Similar with the train_test_split, `random_state` is supplied to ensure consistent results in using the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# simple decision tree training\n",
    "model = DecisionTreeClassifier(random_state=rs)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have just trained your first model. Congratulations! :D\n",
    "\n",
    "> #### `sklearn` models and hyperparameters\n",
    "> When you print out an `sklearn` model, it produces output message as above. It provides information of hyperparameters (i.e. characteristics/settings) which controls behaviour of the model. Each type of model has many hyperparameters and you can find explainations for each in sklearn's very comprehensive documentation. However, not all of them are relevant for your learning objective, thus we will only discuss the relevant hyperparameters in practicals.\n",
    "\n",
    "> You can find DecisionTreeClassifier's documentation [here](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "\n",
    "Take a closer look on the output of the model fit. It prints out the key model hyperparameters. Some of the crucial ones are\n",
    "1. **Criterion:** A method to evaluate the quality of a split. This model uses **gini** method.\n",
    "2. **Max depth:** The maximum depth of the tree. Deeper models are more complex and have more nodes. This model has no depth limitation, which means it can fit the data really well (a bit too well sometimes - resulting in **overfitting**). In lecture, a decision without `max_depth` restriction is also called **maximal tree**.\n",
    "3. **Min samples leaf:** The minimum number of samples required to be at a leaf node, allowing us to limit the minimum size of a leaf node. This model has min samples leaf of 1, almost no limitation of node leaf.\n",
    "\n",
    "Please note of these hyperparameters. We will soon discuss the importance of them and how to tune them to produce a better model.\n",
    "\n",
    "Once the model is trained, it is important to assess its performance. A common method to check the quality of a predictive classification model is `accuracy`. We can use `.score()` to find out its accuracy against a specific set of data. Start by scoring the model againsts the data that the model is trained on (testing how well the model fits the training data).\n",
    "\n",
    "> **Accuracy**: *Accuracy of a predictive classification model refers to the ability of the model to predict the labels of a dataset correctly. It is calculated by dividing the number of **correct predictions** by the number of **data points*** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy:\", model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the model has managed to learn all of the patterns in training data and is able to predict with 100% accuracy. Does it mean this model works perfectly? Not necessarily. We need to check whether it can replicate the performance on similar, but unseen data i.e. the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test accuracy:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model performs really well on the training dataset, it actually fails to accurately predict data in the test set. It seems like this maximal tree overfits on the training dataset. Therefore, we need to tune the hyperparameters of the model and ensure it can generalise better to the test set to find an **optimal tree**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tips: As an alternative to accuracy, `classification_report` can be used to assess performance of a classifier. Classification report outputs a number of statistics for each target class:\n",
    "> 1. Precision: Proportion of all positive predictions that are correct. Precision is a measure of how many positive predictions were actual positive observations.\n",
    "> 2. Recall: Proportion of all real positive observations that are correct. Precision is a measure of how many actual positive observations were predicted correctly.\n",
    "> 3. F1: The harmonic mean of precision and recall. F1 score is an 'average' of both precision and recall.\n",
    "> 4. Support: Number of instances in each class.\n",
    "\n",
    "> Read more: https://chrisalbon.com/machine-learning/precision_recall_and_F1_scores.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding and visualising your decision tree\n",
    "\n",
    "### 3.1. Feature importance\n",
    "\n",
    "Let's take a deeper look on the decision tree that we just built. Firstly, it is important to get insights of the features/input variables that are impactful to the decision making process in our model. This is commonly known as **feature importance**. In an `sklearn` decision tree, feature importance is stored within the model itself in form of **(feature_index, feature_importance_value)**. For easier interpretation, let's match this data with feature names from `X`, sort them in descending order of importance and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# grab feature importances from the model and feature name from the original X\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(importances)\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', importances[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In descending order, the top 3 important variables for this model are `DemMedHomeValue`, `DemIncome` and `DemAge`. Feature importance is really important to not only understand the model, but also to learn more about the data and present conclusions to stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Visualising decision tree structure\n",
    "\n",
    "Other than feature importance, we could also gain insights of our decision tree by visualising it. To do this, use the `export_graphviz` function and `pydot` module. Open the `.png` file to view the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# visualize\n",
    "dotfile = StringIO()\n",
    "export_graphviz(model, out_file=dotfile, feature_names=X.columns)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph[0].write_png(\"week3_dt_viz.png\") # saved in the following file - will return True if successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process will take a long time and your picture will be very large and incomprehensible (we will not show the picture on this practical note due to size restriction). This shows that the model is very complex and deep, which is a typical characteristic of an overfitting model.\n",
    "\n",
    "Let's limit the complexity of the model by setting the `max_depth` that decides maximum length of a branch in the model. By specifying `max_depth` value, branches in decision tree will be stunted once they reached a certain upper limit (also called **pre-pruning** in the lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain with a small max_depth limit\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=3, random_state=rs)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could see that the simpler model (small `max_depth`) actually performs much better on the test data. `max_depth` on a decision tree is what we call a hyperparameter and they are responsible for the structure of a model. Different combinations of hyperparameters will produce different models with different performance too.\n",
    "\n",
    "> #### On hyperparameters vs parameters\n",
    "> In machine learning/data mining community, there is a clear distinction between hyperparameters vs parameters, especially in the context of a model. **Hyperparameters** typically refer to settings of a model that is specified before training, such as maximal depth, splitting criterion and min samples leaf. On the other hand, **parameters** is characteristic of a model that is learned during training process. For decision tree, examples of parameters including number of leaf, splitting rule and feature importance.\n",
    "\n",
    "Let's do a feature importance analysis and visualization on this new decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab feature importance from the model and feature name from the original X\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(importances)\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', importances[i])\n",
    "\n",
    "# visualize\n",
    "dotfile = StringIO()\n",
    "export_graphviz(model, out_file=dotfile, feature_names=X.columns)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph[0].write_png(\"week3_dt_viz.png\") # saved in the following file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Simple decision tree structure](week3_dt_viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree starts splitting with `GiftCnt36`, with `DemMedHomeValue` and `GiftTimeLast` as the competing splits. We have around 8 leaf nodes here, and you can see the number of samples and value splits in each node (e.g. for the right most leaf, we have 1490 donors here, with distribution of 698 non-donors to 792 donors).\n",
    "\n",
    "### 3.3. Visualising relationship between hyperparameters and model performance\n",
    "\n",
    "As we have seen earlier, while a deep, complex decision tree fits the training dataset very well, it actually performs much worse on unseen, test data. In comparison, a shallow, simpler decision tree performs better on the test data. This gap between performance on training dataset versus performance on test dataset can be used as an indication of overfitting.\n",
    "\n",
    "For easier detection of overfitting, we can plot the performance of model with different complexity levels on training and test data. In this practical, we will see the effect of max depth on model performance. As discussed, larger max_depth results in more complex models. Use the following code to produce the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = []\n",
    "train_score = []\n",
    "\n",
    "# check the model performance for max depth from 2-20\n",
    "for max_depth in range(2, 21):\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth, random_state=rs)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_score.append(model.score(X_test, y_test))\n",
    "    train_score.append(model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot max depth hyperparameter values vs training and test accuracy score\n",
    "plt.plot(range(2, 21), train_score, 'b', range(2,21), test_score, 'r')\n",
    "plt.xlabel('max_depth\\nBlue = training acc. Red = test acc.')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the resulting plot, we can see the effect of complexity to overfitting. More complex models produce better training accuracy. However, they also result in lower test data accuracy and larger gap between training/test performance, huge indication of overfitting.\n",
    "\n",
    "Looking at the plot, we could easily point out that a decision tree with `max_depth` of 2 would produce least overfitting. However, in practice, a model complexity is not only determined by one hyperparameters. For a decision tree, it is controlled by `max_depth`, `min_samples_leaf`, `max_features` and many more. Combinations of these hyperparameters produce different models, which makes it difficult to determine the optimal model manually. The next section will guide you to find the optimal hyperparameters in a systematic fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding optimal hyperparameters with GridSearchCV\n",
    "\n",
    "Let's come back to the question of producing a model that does not overfit. How do we find the optimal combination of hyperparameters for a type of model on a given dataset?\n",
    "\n",
    "A common method to find this optimal combination is to run an exhaustive search over a subset of possible values of hyperparameters to optimise, commonly known as grid search. Assume we would like to find the optimal `max_depth` and `min_samples_leaf` for our decision tree. We would like to try maximum of [5, 10, 20] depth and [10, 20, 30] minimum samples in a leaf. Grid search creates combinations of these two parameters (3x3 = 9 possible combinations), build models with these parameter combinations, evaluate the performance on **validation** dataset and choose the one that perform the best.\n",
    "\n",
    "In performing grid search, k-fold cross validation is commonly used. With this approach, each hyperparameter combination is trained and validated in $k$-fold CV fashion, and the average performance values produced are used to determine the best hyperparameter combination.\n",
    "\n",
    "In `sklearn`, grid search + k-fold validation is implemented in `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a GridSearchCV, we first have to determine the hyperparameters and possible values to explore. Each class of predictive models have different kind of hyperparameters (e.g. decision tree will be different to a regression). For this practical, we will focus on 3 hyperparameters:\n",
    "\n",
    "1. Criterion: The function to measure the quality of a split. There are two criterias we will use, “gini” for the Gini impurity and “entropy” for the information gain.\n",
    "2. Max depth: The maximum depth of the tree. From the plot produced in section 3.3., the model performs very well at lower max_depth. Thus, start with range of 2-7.\n",
    "3. Min samples leaf: The minimum number of samples required to be at a leaf node, allowing us to limit the minimum size of a leaf node. Use the range of 20-50 with step of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search CV\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(2, 7),\n",
    "          'min_samples_leaf': range(20, 60, 10)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the test `accuracy` of our model is hugely improved over the previous best (small decision tree in section 3.2.). The best hyperparameters produced by the GridSearchCV provides us with clues of direction to optimise. Let's do another grid search, now being more specific around the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grid search CV #2\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(2, 6),\n",
    "          'min_samples_leaf': range(45, 56)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the second CV is finished, we could see the optimal hyperparameter as follows:\n",
    "* Criterion: entropy\n",
    "* Max depth: 3\n",
    "* Min samples leaf: 52\n",
    "\n",
    "Save this configuration as our **optimal tree**.\n",
    "\n",
    "Try to experiment with other kinds of hyperparameters and see whether you could do better than this. You can find the list of hyperparameters available in a decision tree in `sklearn`'s `DecisionTreeClassifier` documentation website.\n",
    "\n",
    "## Tips\n",
    "\n",
    "1. Always perform feature importance and visualization to understand your decision tree. The model produced by the optimal set of hyperparameters can be found in `.best_estimator_`.\n",
    "2. We will use feature importance a lot across this decision tree modelling process. Rather than writing the script multiple times (which is tedious and error-prone), we can just wrap them in functions in `dm_tools.py` and import it from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inside `dm_tools.py' together with data_prep()\n",
    "import numpy as np\n",
    "import pydot\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "def analyse_feature_importance(dm_model, feature_names, n_to_display=20):\n",
    "    # grab feature importances from the model\n",
    "    importances = dm_model.feature_importances_\n",
    "    \n",
    "    # sort them out in descending order\n",
    "    indices = np.argsort(importances)\n",
    "    indices = np.flip(indices, axis=0)\n",
    "\n",
    "    # limit to 20 features, you can leave this out to print out everything\n",
    "    indices = indices[:n_to_display]\n",
    "\n",
    "    for i in indices:\n",
    "        print(feature_names[i], ':', importances[i])\n",
    "\n",
    "def visualize_decision_tree(dm_model, feature_names, save_name):\n",
    "    dotfile = StringIO()\n",
    "    export_graphviz(dm_model, out_file=dotfile, feature_names=feature_names)\n",
    "    graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "    graph[0].write_png(save_name) # saved in the following file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the feature importance and visualization analysis on GridSearchCV's best model\n",
    "from dm_tools import analyse_feature_importance, visualize_decision_tree\n",
    "\n",
    "analyse_feature_importance(cv.best_estimator_, X.columns, 20)\n",
    "visualize_decision_tree(cv.best_estimator_, X.columns, \"optimal_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GridSearchCV decision tree](optimal_tree.png)\n",
    "\n",
    "## End notes\n",
    "\n",
    "This practical introduces the concepts and techniques of data partitioning. Using the training/test data, we built decision trees and evaluate performance. We analysed feature importance and visualised the decision tree structure. Upon discovering overfitting, we optimised the hyperparameters of the decision trees using GridSearchCV and managed to improve its performance on test dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
