{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apyori in /Users/gunasegarranmagadevan/opt/anaconda3/lib/python3.7/site-packages (1.1.2)\r\n"
     ]
    }
   ],
   "source": [
    "### Installs ###\n",
    "!pip install apyori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "#Clustering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from apyori import apriori\n",
    "\n",
    "# String Processing\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Sequence Rule Mining\n",
    "from collections import defaultdict\n",
    "import subprocess\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS ###\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "    \n",
    "def convert_apriori_results_to_pandas_df(results):\n",
    "    rules = []\n",
    "    \n",
    "    for rule_set in results:\n",
    "        for rule in rule_set.ordered_statistics:\n",
    "            # items_base = left side of rules, items_add = right side\n",
    "            # support, confidence and lift for respective rules\n",
    "            rules.append([','.join(rule.items_base), ','.join(rule.items_add),\n",
    "                         rule_set.support, rule.confidence, rule.lift]) \n",
    "    \n",
    "    # typecast it to pandas df\n",
    "    return pd.DataFrame(rules, columns=['Left_side', 'Right_side', 'Support', 'Confidence', 'Lift']) \n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    tag = {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "    }.get(tag[0], wn.NOUN)\n",
    "\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def cab_tokenizer(document):\n",
    "    # initialize token list\n",
    "    tokens = []\n",
    "    \n",
    "    # split the document into sentences\n",
    "    for sent in sent_tokenize(document):\n",
    "        # split the document into tokens and then create part of speech tag for each token\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            # preprocess and remove unnecessary characters\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If stopword, ignore token and continue\n",
    "            if token in stopwords:\n",
    "                continue\n",
    "\n",
    "            # If punctuation, ignore token and continue\n",
    "            if all(char in punct for char in token):\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token and add back to the tokens list\n",
    "            lemma = lemmatize(token, tag)\n",
    "            tokens.append(lemma)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# function to visualise text cluster. Useful for the assignment too :)\n",
    "def visualise_text_cluster(n_clusters, cluster_centers, terms, num_word = 5):\n",
    "    # -- Params --\n",
    "    # cluster_centers: cluster centers of fitted/trained KMeans/other centroid-based clustering\n",
    "    # terms: terms used for clustering\n",
    "    # num_word: number of terms to show per cluster. Change as you please.\n",
    "    \n",
    "    # find features/terms closest to centroids\n",
    "    ordered_centroids = cluster_centers.argsort()[:, ::-1]\n",
    "    \n",
    "    for cluster in range(n_clusters):\n",
    "        print(\"Top terms for cluster {}:\".format(cluster), end=\" \")\n",
    "        for term_idx in ordered_centroids[cluster, :5]:\n",
    "            print(terms[term_idx], end=', ')\n",
    "            \n",
    "# creating tf-idf terms - a bit slow, do it occasionaly\n",
    "def calculate_tf_idf_terms(document_col):\n",
    "    # Param - document_col: collection of raw document text that you want to analyse\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    # use count vectorizer to find TF and DF of each term\n",
    "    count_vec = CountVectorizer(tokenizer=cab_tokenizer, ngram_range=(1,2))\n",
    "    X_count = count_vec.fit_transform(df['Text'])\n",
    "    \n",
    "    # create list of terms and their tf and df\n",
    "    terms = [{'term': t, 'idx': count_vec.vocabulary_[t],\n",
    "              'tf': X_count[:, count_vec.vocabulary_[t]].sum(),\n",
    "              'df': X_count[:, count_vec.vocabulary_[t]].count_nonzero()}\n",
    "             for t in count_vec.vocabulary_]\n",
    "    \n",
    "    return terms\n",
    "\n",
    "# visualisation of ZIPF law\n",
    "def visualise_zipf(terms, itr_step = 50):\n",
    "    from scipy.spatial.distance import euclidean\n",
    "    from math import sqrt\n",
    "    \n",
    "    # --- Param ---\n",
    "    # terms: collection of terms dictionary from calculate_tf_idf_terms function\n",
    "    # itr_step: used to control how many terms that you want to plot. Num of terms to plot = N terms / itr_step\n",
    "    \n",
    "    # sort terms by its frequency\n",
    "    terms.sort(key=lambda x: (x['tf'], x['df']), reverse=True)\n",
    "    \n",
    "    # select a few of the terms for plotting purpose\n",
    "    sel_terms = [terms[i] for i in range(0, len(terms), itr_step)]\n",
    "    labels = [term['term'] for term in sel_terms]\n",
    "    \n",
    "    # plot term frequency ranking vs its DF\n",
    "    plt.plot(range(len(sel_terms)), [x['df'] for x in sel_terms])\n",
    "    plt.xlabel('Term frequency ranking')\n",
    "    plt.ylabel('Document frequency')\n",
    "    \n",
    "    max_x = len(sel_terms)\n",
    "    max_y = max([x['df'] for x in sel_terms])\n",
    "    \n",
    "    # annotate the points\n",
    "    prev_x, prev_y = 0, 0\n",
    "    for label, x, y in zip(labels,range(len(sel_terms)), [x['df'] for x in sel_terms]):\n",
    "        # calculate the relative distance between labels to increase visibility\n",
    "        x_dist = (abs(x - prev_x) / float(max_x)) ** 2\n",
    "        y_dist = (abs(y - prev_y) / float(max_y)) ** 2\n",
    "        scaled_dist = sqrt(x_dist + y_dist)\n",
    "        \n",
    "        if (scaled_dist > 0.1):\n",
    "            plt.text(x+2, y+2, label, {'ha': 'left', 'va': 'bottom'}, rotation=30)\n",
    "            prev_x, prev_y = x, y\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "''' Uses SPMF to find association rules in supplied transactions '''\n",
    "def get_association_rules(sequences, min_sup, min_conf):\n",
    "    # step 1: create required input for SPMF\n",
    "    \n",
    "    # prepare a dict to uniquely assign each item in the transactions to an int ID\n",
    "    item_dict = defaultdict(int)\n",
    "    output_dict = defaultdict(str)\n",
    "    item_id = 1\n",
    "    \n",
    "    # write your sequences in SPMF format\n",
    "    with open('seq_rule_input.txt', 'w+') as f:\n",
    "        for sequence in sequences:\n",
    "            z = []\n",
    "            for itemset in sequence:\n",
    "                # if there are multiple items in one itemset\n",
    "                if isinstance(itemset, list):\n",
    "                    for item in itemset:\n",
    "                        if item not in item_dict:\n",
    "                            item_dict[item] = item_id\n",
    "                            item_id += 1\n",
    "\n",
    "                        z.append(item_dict[item])\n",
    "                else:\n",
    "                    if itemset not in item_dict:\n",
    "                        item_dict[itemset] = item_id\n",
    "                        output_dict[str(item_id)] = itemset\n",
    "                        item_id += 1\n",
    "                    z.append(item_dict[itemset])\n",
    "                    \n",
    "                # end of itemset\n",
    "                z.append(-1)\n",
    "            \n",
    "            # end of a sequence\n",
    "            z.append(-2)\n",
    "            f.write(' '.join([str(x) for x in z]))\n",
    "            f.write('\\n')\n",
    "    \n",
    "    # run SPMF with supplied parameters\n",
    "    supp_param = '{}%'.format(int(min_sup * 100))\n",
    "    conf_param = '{}%'.format(int(min_conf * 100))\n",
    "    subprocess.call(['java', '-jar', 'spmf.jar', 'run', 'RuleGrowth', 'seq_rule_input.txt', 'seq_rule_output.txt', '10%', '10%'], shell=True)\n",
    "    \n",
    "    # read back the output rules\n",
    "    outputs = open('seq_rule_output.txt', 'r').read().strip().split('\\n')\n",
    "    output_rules = []\n",
    "    for rule in outputs:\n",
    "        left, right, sup, conf = re.search(pattern=r'([0-9\\,]+) ==> ([0-9\\,]+) #SUP: ([0-9]+) #CONF: ([0-9\\.]+)', string=rule).groups()\n",
    "        sup = int(sup) / len(sequences)\n",
    "        conf = float(conf)\n",
    "        output_rules.append([[output_dict[x] for x in left.split(',')], [output_dict[x] for x in right.split(',')], sup, conf])\n",
    "    \n",
    "    # return pandas DataFrame\n",
    "    return pd.DataFrame(output_rules, columns = ['Left_rule', 'Right_rule', 'Support', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PRE-PROCESSING ###\n",
    "def _PerpData (df):\n",
    "    \n",
    "    # Get the names of the columns\n",
    "    _names = list(df)\n",
    "    \n",
    "    # loop through the data and change all null to np.nan\n",
    "    for _name in _names:\n",
    "        df[_name] = df[_name].replace('', np.nan)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LocX</th>\n",
       "      <th>LocY</th>\n",
       "      <th>RegDens</th>\n",
       "      <th>RegPop</th>\n",
       "      <th>MedHHInc</th>\n",
       "      <th>MeanHHSz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00601</td>\n",
       "      <td>-66.749472</td>\n",
       "      <td>18.180103</td>\n",
       "      <td>70</td>\n",
       "      <td>19143</td>\n",
       "      <td>9888</td>\n",
       "      <td>3.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00602</td>\n",
       "      <td>-67.180247</td>\n",
       "      <td>18.363285</td>\n",
       "      <td>83</td>\n",
       "      <td>42042</td>\n",
       "      <td>11384</td>\n",
       "      <td>3.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00603</td>\n",
       "      <td>-67.134224</td>\n",
       "      <td>18.448619</td>\n",
       "      <td>86</td>\n",
       "      <td>55592</td>\n",
       "      <td>10748</td>\n",
       "      <td>2.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00604</td>\n",
       "      <td>-67.136995</td>\n",
       "      <td>18.498987</td>\n",
       "      <td>83</td>\n",
       "      <td>3844</td>\n",
       "      <td>31199</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00606</td>\n",
       "      <td>-66.958807</td>\n",
       "      <td>18.182151</td>\n",
       "      <td>65</td>\n",
       "      <td>6449</td>\n",
       "      <td>9243</td>\n",
       "      <td>3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33173</th>\n",
       "      <td>99927</td>\n",
       "      <td>-133.606896</td>\n",
       "      <td>56.337957</td>\n",
       "      <td>15</td>\n",
       "      <td>136</td>\n",
       "      <td>17143</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33174</th>\n",
       "      <td>99929</td>\n",
       "      <td>-132.338228</td>\n",
       "      <td>56.409507</td>\n",
       "      <td>5</td>\n",
       "      <td>2424</td>\n",
       "      <td>43696</td>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33175</th>\n",
       "      <td>99950</td>\n",
       "      <td>-131.466339</td>\n",
       "      <td>55.875767</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>37708</td>\n",
       "      <td>2.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33176</th>\n",
       "      <td>999HH</td>\n",
       "      <td>-132.288850</td>\n",
       "      <td>55.539963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33177</th>\n",
       "      <td>999XX</td>\n",
       "      <td>-132.549366</td>\n",
       "      <td>55.616026</td>\n",
       "      <td>1</td>\n",
       "      <td>382</td>\n",
       "      <td>33250</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33178 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID        LocX       LocY RegDens  RegPop  MedHHInc  MeanHHSz\n",
       "0      00601  -66.749472  18.180103      70   19143      9888      3.24\n",
       "1      00602  -67.180247  18.363285      83   42042     11384      3.10\n",
       "2      00603  -67.134224  18.448619      86   55592     10748      2.84\n",
       "3      00604  -67.136995  18.498987      83    3844     31199      3.00\n",
       "4      00606  -66.958807  18.182151      65    6449      9243      3.20\n",
       "...      ...         ...        ...     ...     ...       ...       ...\n",
       "33173  99927 -133.606896  56.337957      15     136     17143      2.03\n",
       "33174  99929 -132.338228  56.409507       5    2424     43696      2.49\n",
       "33175  99950 -131.466339  55.875767       2      47     37708      2.14\n",
       "33176  999HH -132.288850  55.539963     NaN       0         0      0.00\n",
       "33177  999XX -132.549366  55.616026       1     382     33250      2.43\n",
       "\n",
       "[33178 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### MAIN ###\n",
    "# Read the Data\n",
    "data = pd.read_csv('lab3.csv', na_filter=False)\n",
    "data = _PerpData(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Clustering</h1>\n",
    "<h3>K-Means<h3/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-66b13b25825c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# set the random state. different random state seeds might result in different centroids locations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# sum of intra-cluster distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"C\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n\u001b[0;32m--> 859\u001b[0;31m                         order=order, copy=self.copy_x)\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "### KMeans ###\n",
    "# take 3 variables and drop the rest\n",
    "data_kmeans = data[['MedHHInc', 'MeanHHSz', 'RegDens']]\n",
    "\n",
    "# convert df2 to matrix\n",
    "X = np.matrix(data_kmeans)\n",
    "\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# random state, we will use 42 instead of 10 for a change\n",
    "rs = 42\n",
    "\n",
    "# set the random state. different random state seeds might result in different centroids locations\n",
    "model = KMeans(n_clusters=3, random_state=rs)\n",
    "model.fit(X)\n",
    "\n",
    "# sum of intra-cluster distances\n",
    "print(\"Sum of intra-cluster distance:\", model.inertia_)\n",
    "\n",
    "print(\"Centroid locations:\")\n",
    "for centroid in model.cluster_centers_:\n",
    "    print(centroid)\n",
    "    \n",
    "# set a different n_clusters\n",
    "model = KMeans(n_clusters=8, random_state=rs)\n",
    "model.fit(X)\n",
    "\n",
    "# sum of intra-cluster distances\n",
    "print(\"Sum of intra-cluster distance:\", model.inertia_)\n",
    "\n",
    "print(\"Centroid locations:\")\n",
    "for centroid in model.cluster_centers_:\n",
    "    print(centroid)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Agglomerative Clustering | (Alternative to Kmeans Clustering)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_model = AgglomerativeClustering(n_clusters=3)\n",
    "agg_model.fit(X[:50]) # subset of X, only 50 data points\n",
    "\n",
    "plot_dendrogram(agg_model, labels=agg_model.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Understanding and Visualising a Clustering Model</h3>\n",
    "<h4>Method 1: Pair Plots</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=3, random_state=rs).fit(X)\n",
    "\n",
    "# assign cluster ID to each record in X\n",
    "# Ignore the warning, does not apply to our case here\n",
    "y = model.predict(X)\n",
    "data_kmeans['Cluster_ID'] = y\n",
    "\n",
    "# how many records are in each cluster\n",
    "print(\"Cluster membership\")\n",
    "print(data_kmeans['Cluster_ID'].value_counts())\n",
    "\n",
    "# pairplot the cluster distribution.\n",
    "cluster_g = sns.pairplot(data_kmeans, hue='Cluster_ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Method 2: Distribution Plots</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the column and bin size. Increase bin size to be more specific, but 20 is more than enough\n",
    "cols = ['MedHHInc', 'MeanHHSz', 'RegDens']\n",
    "n_bins = 20\n",
    "\n",
    "# inspecting cluster 0 and 1\n",
    "clusters_to_inspect = [0,1]\n",
    "\n",
    "for cluster in clusters_to_inspect:\n",
    "    # inspecting cluster 0b\n",
    "    print(\"Distribution for cluster {}\".format(cluster))\n",
    "\n",
    "    # create subplots\n",
    "    fig, ax = plt.subplots(nrows=3)\n",
    "    ax[0].set_title(\"Cluster {}\".format(cluster))\n",
    "\n",
    "    for j, col in enumerate(cols):\n",
    "        # create the bins\n",
    "        bins = np.linspace(min(data_kmeans[col]), max(data_kmeans[col]), 20)\n",
    "        # plot distribution of the cluster using histogram\n",
    "        sns.distplot(data_kmeans[data_kmeans['Cluster_ID'] == cluster][col], bins=bins, ax=ax[j], norm_hist=True)\n",
    "        # plot the normal distribution with a black line\n",
    "        sns.distplot(data_kmeans[col], bins=bins, ax=ax[j], hist=False, color=\"k\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Determine K</h3>\n",
    "<h4>Method 1: Elbow method </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to save the clusters and cost\n",
    "clusters = []\n",
    "inertia_vals = []\n",
    "\n",
    "# this whole process should take a while\n",
    "for k in range(2, 15, 2):\n",
    "    # train clustering with the specified K\n",
    "    model = KMeans(n_clusters=k, random_state=rs, n_jobs=10)\n",
    "    model.fit(X)\n",
    "    \n",
    "    # append model to cluster list\n",
    "    clusters.append(model)\n",
    "    inertia_vals.append(model.inertia_)\n",
    "\n",
    "# plot the inertia vs K values\n",
    "plt.plot(range(2,15,2), inertia_vals, marker='*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Method 2: Silhoette Score </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clusters[1])\n",
    "print(\"Silhouette score for k=4\", silhouette_score(X, clusters[1].predict(X)))\n",
    "\n",
    "print(clusters[2])\n",
    "print(\"Silhouette score for k=6\", silhouette_score(X, clusters[2].predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Performing Association Mining</h2>\n",
    "<h3>Apriori algorithm</h3>\n",
    "<h4>Create Transactions</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by account, then list all services\n",
    "transactions = df.groupby(['GROUP_BY_FIELD'])['PROPERTY_FIELD'].apply(list)\n",
    "\n",
    "print(transactions.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Possible Parameters</u></b>\n",
    "\n",
    "1. `transactions`: list of list of items in transactions (eg. [['A', 'B'], ['B', 'C']]).\n",
    "2. `min_support`: Minimum support of relations in float percentage. It specifies a minimum level of support to claim that items are associated (i.e. they occur together in the dataset). Default 0.1.\n",
    "3. `min_confidence`: Minimum confidence of relations in float percentage. Default 0.0.\n",
    "4. `min_lift`: Minimum lift of relations in float percentage. Default 0.0.\n",
    "5. `max_length`: Max length of the relations. Default None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type cast the transactions from pandas into normal list format and run apriori\n",
    "transaction_list = list(transactions)\n",
    "results = list(apriori(transaction_list, min_support=0.05))\n",
    "\n",
    "# print first 5 rules\n",
    "print(results[:5])\n",
    "\n",
    "result_df = convert_apriori_results_to_pandas_df(results)\n",
    "\n",
    "print(result_df.head(20))\n",
    "\n",
    "## Get the most appropriate variables ##\n",
    "# sort all acquired rules descending by lift\n",
    "result_df = result_df.sort_values(by='Lift', ascending=False)\n",
    "print(result_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Mining</h2>\n",
    "<h3>Data reading</h3>\n",
    "<h4>Loading Text</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_json('datasets/Federalistpapers.json')\n",
    "\n",
    "# random state\n",
    "rs = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Data Exploration</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as usual, explore the dataset\n",
    "df.info()\n",
    "\n",
    "# print out the first 200 characters of the first row of text column\n",
    "print(df.get_value(index=0, col='Text')[:200])\n",
    "\n",
    "# average length of text column\n",
    "print(df['Text'].apply(lambda x: len(x)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Prepocessing</h3>\n",
    "<h4>Bag Of Words</h4>\n",
    "\n",
    "1. **Lowercase**: cast each word into its lowercased version. Ensure differently capitalised words are treated as the same word.\n",
    "2. **Punctuation** removal: remove all punctuation marks. We are only interested at the words.\n",
    "3. **Part of speech** filtering: keep tokens with certain part-of-speech only. In particular, here we are interested in adjectives, adverbs, nouns and verbs.\n",
    "4. **Lemmatisation**: reducing words/tokens into their base form. \n",
    "\n",
    "> #### Lemmatisation vs stemming\n",
    ">   There are minor diffrences between Lemmatisation and **stemming** that was\n",
    ">   introduced in the lecture. Stemming usually refers to a heuristic process\n",
    ">   that chops off the ends of words to get the base forms of the words.\n",
    ">   Lemmatisation usually refers the process of doing similar things, but using\n",
    ">   additional information of how the word is used in the context (semantic\n",
    ">   information). Lemmatisation returns the dictionary form of a word, also\n",
    ">   known as lemma. Given the word \"ponies\" for example, stemming will return\n",
    ">   \"poni\" (incorrect), while lemmatisation will return \"pony\" (correct). The\n",
    ">   drawback of lemmatisation is it requires additional analysis to obtain the\n",
    ">   required semantic information.\n",
    ">   [Source](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise WordNet lemmatizer and punctuation filter\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "# load the provided stopwords\n",
    "df_stop = pd.read_json('datasets/Federaliststop.json')\n",
    "\n",
    "# join provided stopwords with the default NLTK English stopwords\n",
    "stopwords = set(df_stop['Term']).union(set(sw.words('english')))\n",
    "\n",
    "# tf idf vectoriser\n",
    "tfidf_vec = TfidfVectorizer(tokenizer=cab_tokenizer, ngram_range=(1,2))\n",
    "X = tfidf_vec.fit_transform(df['Text'])\n",
    "\n",
    "# see the number of unique tokens produced by the vectorizer. Lots of them...\n",
    "print(len(tfidf_vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Document Analysis</h3>\n",
    "<h4>Kmeans</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Elbow and sollouet rule to get optimal K\n",
    "# {Not Included}\n",
    "# K means clustering using the term vector\n",
    "kmeans = KMeans(n_clusters=7, random_state=rs).fit(X)\n",
    "\n",
    "# visualize it\n",
    "visualise_text_cluster(kmeans.n_clusters, kmeans.cluster_centers_, tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Selection and Transformation</h3>\n",
    "<h4>Zipf's Law and Document Frequency Filtering</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = calculate_tf_idf_terms(df['Text'])\n",
    "\n",
    "visualise_zipf(terms)\n",
    "\n",
    "# another tf idf vectoriser\n",
    "# limit the terms produced to terms that occured in min of 2 documents and max 80% of all documents\n",
    "filter_vec = TfidfVectorizer(tokenizer=cab_tokenizer, ngram_range=(1,2), min_df=2, max_df=0.8)\n",
    "X_filter = filter_vec.fit_transform(df['Text'])\n",
    "\n",
    "# see the number of unique tokens produced by the vectorizer. Reduced!\n",
    "print(len(filter_vec.get_feature_names()))\n",
    "\n",
    "## TESTING THE FILTER ##\n",
    "# K means clustering using the new term vector, time it for comparison to SVD\n",
    "kmeans_fil = KMeans(n_clusters=7, random_state=rs).fit(X_filter)\n",
    "# visualisation\n",
    "visualise_text_cluster(kmeans_fil.n_clusters, kmeans_fil.cluster_centers_, filter_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Singlular Value Decompisition</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_trans = svd.fit_transform(X_filter)\n",
    "\n",
    "# sort the components by largest weighted word\n",
    "sorted_comp = svd.components_.argsort()[:, ::-1]\n",
    "terms = filter_vec.get_feature_names()\n",
    "\n",
    "# visualise word - concept/component relationships\n",
    "for comp_num in range(10):\n",
    "    print(\"Top terms in component #{}\".format(comp_num), end=\" \")\n",
    "    for i in sorted_comp[comp_num, :5]:\n",
    "        print(terms[i], end=\", \")\n",
    "    print()\n",
    "    \n",
    "# K-means clustering using LSA-transformed X\n",
    "svd_kmeans = KMeans(n_clusters=7, random_state=rs).fit(X_trans)\n",
    "\n",
    "# transform cluster centers back to original feature space for visualisation\n",
    "original_space_centroids = svd.inverse_transform(svd_kmeans.cluster_centers_)\n",
    "\n",
    "# visualisation\n",
    "visualise_text_cluster(svd_kmeans.n_clusters, original_space_centroids, filter_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sequential Rule Mining Using SPMF</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_association_rules(sequences, 0.1, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
